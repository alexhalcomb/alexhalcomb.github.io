<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Using Satellite Imagery to Identify Solar Panel Arrays by alexhalcomb</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Using Satellite Imagery to Identify Solar Panel Arrays</h1>
      <h2 class="project-tagline">Capstone Project for General Assembly Data Science Immersive</h2>
    </section>

    <section class="main-content">
      <h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>Over the last decade, U.S. residential and commercial solar panel installations have grown dramatically. This is in large part due significant reductions in cost, but also because of growing concerns regarding climate change. In the last ten years, solar installation costs have fallen by over 70% and they continue to improve. </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/solar-price-installation-chart.jpg" alt="U.S. Solar Growth"></p>

<p>The full extent of this growth, however, is not well understood as the data for residential and commercial solar installations is typically reported only at the level of the state. Having a more granular dataset would benefit a number of parties, including utilities, policy makers, and solar developers. Utilities, in particular, will face many challenges integrating such a high amount of distributed generation and therefore will benefit greatly from understanding the magnitude and distribution of these resources. </p>

<p>The goal of this project is to identify solar panel arrays in high resolution satellite imagery using machine learning techniques. Specifically, I will divide images into segments by grouping similar pixels together, calculate features for each image segment, and train a classifier to differentiate solar panel regions from non-solar panel regions. </p>

<h2>
<a id="dataset" class="anchor" href="#dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset</h2>

<p>In this analysis, I will utilize a dataset created by the Energy Data Analytics Lab at Duke University. The dataset contains geospatial data and border vertices for over 19,000 solar panels, located in the California cities of Fresno, Stockton, Oxnard, and Modesto. The full dataset of images and annotations can be found <a href="https://figshare.com/collections/Full_Collection_Distributed_Solar_Photovoltaic_Array_Location_and_Extent_Data_Set_for_Remote_Sensing_Object_Identification/3255643/1">here</a>. </p>

<p>The satellite images are part of the USGS collection of High Resolution Orthoimagery (HRO). The majority of the raw images are 5000x5000 pixels, at a resolution of 0.3m/pixel. Although the USGS High Resolution database does not encompass the entire U.S., it does cover many urban areas in the country and continues to grow as the agency acquires new data.  </p>

<p>Below is a summary of the solar panel annotations for the entire dataset.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/dataset_hist.png" alt="Full Dataset"></p>

<p>Due to size constraints, a subset of the larger dataset was used for this analysis. This subset contains all cities except Fresno.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/dataset_hist_sub.png" alt="Subset for Analysis"></p>

<p>Here you can see one of the satellite images from the dataset, with solar panels plotted in blue. </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/panel_mapping.png" alt="Panels in Image"></p>

<p>To show how this looks on a more granular basis, I've zoomed in on five panels and plotted outlines using the polygon vertices provided in the dataset.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/sample_panels.png" alt="Panels"></p>

<h2>
<a id="image-segmentation" class="anchor" href="#image-segmentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Segmentation</h2>

<p>Image segmentation consists of grouping pixels together based on likeness (e.g. color value, proximity). Relevant information can then be extracted and aggregated across each image segment, instead of from the individual pixels themselves. This approach produces a more meaningful model of the image data and simplifies the classification process. Additionally, segment-based classification can often achieve better accuracy than a pixel-based method.</p>

<p>Below is an example segmentation applied to a satellite image containing a solar panel. As you can see, the image has been partitioned into contiguous regions with similar pixel values. </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/sample_segmentation.png" alt="Segmentation Example"></p>

<p>The satellite image above is only 100x100 pixels, however, so I will modify the segmentation process for the larger 5000x5000 pixel images. In particular, I will use the annotations data to "segment" the solar panel arrays. In the larger images, solar panels make up a tiny fraction of the total area (&lt; 1.0%). Therefore, to generate the segments that do not contain solar panels, I will sample sections of the image and run the segmentation algorithm on these sections. </p>

<p>The panel segments and the non-panel sections (colored squares) for a sample image are shown below.  </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/image_panel_mask.png" alt="Panel / Non-Panel Mask"></p>

<h2>
<a id="feature-extraction" class="anchor" href="#feature-extraction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature Extraction</h2>

<p>Feature extraction is a key component of image analysis and object detection. I aim to classify segments or superpixels and will therefore calculate features based on these regions of pixels, rather than the pixels themselves. Once a feature set is established, I will then be able to train a classifier to identify regions that are best representative of solar panels. </p>

<p>In this analysis, features will mostly consist of summary statistics for each color channel (e.g. mean, min, max, variance) but will also include other descriptive values such as area and perimeter.    </p>

<p>I first define a function to extract spatial features such as area and perimeter. Additionally, I use area and perimeter to calculate a new feature 'circleness' which measures how circular a region is. A value of 1 indicates a perfect circle, whereas a square is ~.80.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">def</span> <span class="pl-en">segment_features_shape</span>(<span class="pl-smi">segments</span>):

    <span class="pl-c"># For each region/segment, create regionprops object and extract features</span>
    region_props_all <span class="pl-k">=</span> measure.regionprops(segments)    
    region_features <span class="pl-k">=</span> {}

    <span class="pl-k">for</span> i, region <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(region_props_all):    
        shape_features <span class="pl-k">=</span> {}

        shape_features[<span class="pl-s"><span class="pl-pds">'</span>perimeter<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> region.perimeter
        shape_features[<span class="pl-s"><span class="pl-pds">'</span>area<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> region.area
        shape_features[<span class="pl-s"><span class="pl-pds">'</span>circleness<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> (<span class="pl-c1">4</span><span class="pl-k">*</span>np.pi<span class="pl-k">*</span>region.area) <span class="pl-k">/</span> (<span class="pl-c1">max</span>(region.perimeter,<span class="pl-c1">1</span>)<span class="pl-k">**</span><span class="pl-c1">2</span>)
        shape_features[<span class="pl-s"><span class="pl-pds">'</span>centroid<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> region.centroid
        shape_features[<span class="pl-s"><span class="pl-pds">'</span>coords<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> region.coords

        region_features[i] <span class="pl-k">=</span> shape_features

    shape_df <span class="pl-k">=</span> pd.DataFrame(region_features)

    <span class="pl-k">return</span> shape_df</pre></div>

<p>Next, I define a function to extract the color related features. This function iterates through the red, green, and blue channels of the image region and calculates the relevant values.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">def</span> <span class="pl-en">segment_features_color</span>(<span class="pl-smi">img</span>, <span class="pl-smi">segments</span>):

    <span class="pl-c"># Generate unique labels list</span>
    segment_labels <span class="pl-k">=</span> np.unique(segments[segments <span class="pl-k">&gt;</span> <span class="pl-c1">0</span>])

    <span class="pl-c"># For each segment and channel, calculate summary stats    </span>
    channels <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">'</span>r<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>g<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>b<span class="pl-pds">'</span></span>]    
    region_features <span class="pl-k">=</span> {}

    <span class="pl-k">for</span> label <span class="pl-k">in</span> segment_labels:
        region <span class="pl-k">=</span> img[segments <span class="pl-k">==</span> label]        
        color_features <span class="pl-k">=</span> {}        

        <span class="pl-k">for</span> i, channel <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(channels):
            values <span class="pl-k">=</span> scipy.stats.describe(region[:,i])
            color_features[channel <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>_min<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> values.minmax[<span class="pl-c1">0</span>]
            color_features[channel <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>_max<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> values.minmax[<span class="pl-c1">1</span>]
            color_features[channel <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>_mean<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> values.mean
            color_features[channel <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">'</span>_variance<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> values.variance     

        region_features[label] <span class="pl-k">=</span> color_features

    color_df <span class="pl-k">=</span> pd.DataFrame(region_features)

    <span class="pl-k">return</span> color_df</pre></div>

<h1>
<a id="classification" class="anchor" href="#classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Classification</h1>

<h3>
<a id="logistic-regression" class="anchor" href="#logistic-regression" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Logistic Regression</h3>

<p>I've chosen to use Logistic Regression due to its simplicity and interpretability. The initial results are encouraging, with an accuracy of 96% on the testing data, compared to a baseline accuracy of 91%. As seen in the graph of coefficient values below, the features that are most meaningful are the mean color values, area, and circleness.  </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/LR_coeffs.png" height="400," align="center"> </p>

<h3>
<a id="random-forest" class="anchor" href="#random-forest" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Random Forest</h3>

<p>Decision Tree models are comprised of a series of nodes, or decisions, where an element of the data is tested e.g. x &gt; 5. These tests are derived by evaluating how effective they are at dividing the data into homogenous groups. A Random Forest is simply a collection of Decision Tree models, where each Decision Tree is fit on a random sample of the data. Random Forests are very powerful and are adept at learning complicated, non-linear relationships. </p>

<p>By using many Decision Trees on multiple versions of the dataset, overfitting can be avoided. This reduces model variance significantly with only a small increase in bias. This is a key advantage of using ensemble methods. For this reason, I've chosen to use a Random Forest Classifier. The Random Forest outperforms the Logistic Regression, with an accuracy of 98% compared to a baseline accuracy of 91%. As with the Logistic Regression, the Random Forest placed high importance on the area and circleness features. However, the Random Forest feature importances show that the most important features were the color variances.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/RF_feat_importance.png" height="400," align="center"> </p>

<p>To evaluate the performance of this classifier, I will compare the model predictions against the true values. When the dataset is strongly imbalanced, as is the case here, this can be a much more intuitive way to analyze the results. To visualize this I will use a Confusion Matrix, which shows the proportion of correct/incorrect model predictions. </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/CMs.png" alt="Confusion Matrices"></p>

<p>The Receiver Operating Characteristic curve measures the True Positive Rate or Recall (TP / TP + FN) against the False Positive Rate (FP / FP + TN). There is a tradeoff between and the two and plotting the ROC is a helpful way to analyze the effectiveness of a classifier. </p>

<p>In this analysis, the True Positive Rate measures how many solar panels are being correctly classified, out of the total number of solar panels in the image. As this ratio increases, however, the number of non panel regions that are being misclassified as solar panels also increases. The AUC, or Area Under the Curve, is a common way to quantify this tradeoff. An AUC of 1 reflects perfect classification.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/ROC_curve.png" height="400," align="center"> </p>

<p>The Precision Recall curve is closely related to the ROC curve in that both reflect tradeoffs. Instead of the False Positive Rate, however, Recall is compared to Precision which measures the number of true positives out of total positive predictions (TP / TP + FP). </p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/PR_curve.png" height="400," align="center"> </p>

<h3>
<a id="misclassified" class="anchor" href="#misclassified" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Misclassified</h3>

<p>Around 20% of the solar panels are being misclassified as 'not a solar panel' using the current model. I will examine the false negatives with the lowest predicted probabilities in order to better understand where the model is failing the worst.</p>

<p><img src="https://raw.githubusercontent.com/alexhalcomb/DSI-Capstone-Project/master/assets/misclassified_panels.png" height="200," align="center"> </p>

<h1>
<a id="conclusion-and-next-steps" class="anchor" href="#conclusion-and-next-steps" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion and Next Steps</h1>

<p>The results from this analysis demonstrate that it is possible to successfully use satellite imagery to detect objects of interest. In this case, the pixel characteristics of solar panels uniquely identify them within a large background. Going forward, there are a number of areas for further improvement. </p>

<h4>
<a id="problematic-panels" class="anchor" href="#problematic-panels" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problematic Panels</h4>

<p>It's clear that there are two types of solar panels that have proven difficult to detect. Panels that appear very light in color, and those that show significant grid lines. In the first instance, the panels tend to blend in with the roof of the building and are therefore difficult to distinguish. In the second case, the color values and variance are skewed relative to the rest of the dataset.</p>

<h4>
<a id="images-in-the-wild" class="anchor" href="#images-in-the-wild" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Images in the Wild</h4>

<p>The end goal for this project is to classify solar panels in images that have not been annotated. I have begun the process of generalizing this framework to images 'in the wild'. The key challenges will be as follows:</p>

<ol>
<li>Segmentation - Ensuring that the entirety of a solar panel is included in its own region. Will require fine tuning of segmentation algorithm parameters and potentially pre-segmentation image processing (filters, denoising, etc.).</li>
<li>Modify Classifier - Bridging the gap between the data that the classifier trained on and what will be outputted from a new image. Specifically, solar panel annotations in the ground truth dataset do not necessarily match up with what a segmentation algorithm will extract for a solar panel region. </li>
</ol>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
